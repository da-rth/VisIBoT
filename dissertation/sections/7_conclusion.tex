% Checked with Grammarly - 21/03/2021
% Proof read by Dan

\chapter{Conclusion}

The combined process of automated traffic classification, payload extraction, and sandbox-based malware analysis has proved highly effective for identifying and visualising IoT botnet traffic. VisiBot has shown to be robust, extensible, and capable of scaling demand by employing a globally distributed honeypot network and Message Broker-based analysis system. The distribution of honeypot and malware analysis tasks across multiple workers allows for real-time and parallelised packet classification, payload URL extraction, and malware analysis. The collection of botnet entity IP address meta-data, such as geographic location, Common Vulnerability Exploits, and Autonomous System history, provides sufficient knowledge for visualising and characterising current botnet behaviours and mannerisms, including the ubiquity and density of globally distributed botnets, leading countries of C2 and P2P botnet activity, and ASN trading patterns associated botnet IP addresses.

Classifying over 58,010 unique IP addresses, VisiBot was able to identify, examine, and visualise 1,303 candidate Command \& Control servers and 6,876 Peer-to-Peer nodes associated with 1,654 malware samples over a 35-day evaluation period. Unlike previous implementations, the heuristics used within the VisiBot Processing System permit identifying both centralised and de-centralised botnets through automated sample extraction, sandboxing and a combination of analysis techniques. As the project is written in Python and containerised using docker, its various components are highly extensible. The VisiBot analysis stage can be extended to include additional heuristics and detection for new botnet characteristics, communication protocols, and obfuscation techniques. In its current configuration, the simple heuristics used by VisiBot allowed for the identification and analysis of a relatively new and highly dominant Peer-to-Peer botnet known as Mozi. The system also detected several other botnets, including Mirai, Bashlite, and various unknown variants. However, some issues were encountered throughout the data collection and analysis, such as malware sample unpacking, execution of extracted bash scripts, and the collection of malicious bot samples.

\section{Future Work}

As the proposed system has yet to implement a handshake/validation process, VisiBot cannot determine the legitimacy of detected candidate C2 servers and P2P traffic. The heuristic analysis procedure can be expanded in future iterations to accommodate the automated execution of handshake procedures against such candidates. However, as the handshake procedure often varies between botnets, additional static and dynamic analysis heuristics may be required to infer the type of botnet variant before attempting a handshake. As described by \citet{Bastos2019}, Mirai and Bashlite Command \& Control servers can be validated through reverse engineering a malware sample, inspecting its handshaking procedure, and performing the handshake with the candidate. This validation process is equally necessary for Peer-to-Peer botnets, as Mozi and Hajime purposefully attempt to obfuscate Peer-to-Peer botnet activity through interacting with public networks, protocols, and Distributed Hash Tables used by legitimate peers. \citep{Netlab2019} The DHT handshake technique proposed by \citet{Herwig2019} could also be applied to future iterations of the VisiBot Processing System such that detected P2P Nodes are validated through the automatic exchanging of public keys or other handshake signatures.

Despite proving effective for candidate C2 and P2P detection, some identification heuristics were less effective than others throughout the evaluation period. Namely, the heuristics based on hard-coded IP addresses of malware binaries proved challenging to utilise due to limitations caused by the unpacking of collected malware samples. The VisiBot Processing System had difficulties unpacking several malware binaries packed with tools such as UPX \citep{UPX}, leading to the extraction of obfuscated/nonsensical strings during static analysis. By developing a more robust malware unpacking procedure with improved detection and alternative unpacking mechanisms, we can ensure the constant collection of crucial strings, such as IP addresses, which are actively used throughout the heuristic analysis process. By doing so, heuristics that depend on static analysis information, such as hard-coded strings, will not be hindered by obfuscation techniques such as binary packing.

Additionally, the current identification heuristics can be expanded to utilise additional data sources collected during LiSa analysis. All network traffic is logged within Packet Capture (pcap) files during the LiSa sandboxing stage. The captured packet information can be further analysed using heuristic analysis techniques to increase C2 and P2P detection accuracy. The VisiBot Processing System also collects several log files during sandbox analysis, including program output logs and machine logs. This information can be used to infer botnet characteristics and further increase the detection accuracy of VisiBot. 

Lastly, a significant limitation of the VisiBot Processing System is the honeypot data retrieval process from the Bad Packets \citep{BadPackets} honeypot service, as new results are only made available on an hourly basis. In some cases, this waiting period limits the number of malware samples extracted from remote code execution attempts conducted by malicious bots from botnets such as Mozi. Sample URLs are often hosted on local HTTP servers using temporary ports; thus, extracted URLs have a short and unpredictable life expectancy. This issue may be resolvable in future iterations of VisiBot by employing a stream-based honeypot collection system instead of an HTTP API. The transition to a streaming API would allow packets to be processed as soon as the honeypot network detects/processes them, mitigating any additional waiting time between queries. The immediate processing of honeypot packets would allow for quicker extraction of unpredictable payload URLs, which, as observed, are becoming increasingly more common amongst de-centralised botnets.